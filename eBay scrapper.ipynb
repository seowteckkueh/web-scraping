{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# text-wrap for notebook output\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "8RLlockM2PR0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "stIsHKvdqkBM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a48abb9-43a6-458e-db6e-a3a57e01e8c8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server responded: 404\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TODO\n",
        "1. Make a request to the ebay.com and get a page\n",
        "2. collect data from each detail page\n",
        "3. collect all links to details pages of each product\n",
        "4. write scrapped data to a csv file\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import csv\n",
        "\n",
        "def get_page(url):\n",
        "    response=requests.get(url)\n",
        "\n",
        "    if not response.ok:\n",
        "        print('Server responded:', response.status_code)\n",
        "        pass\n",
        "        \n",
        "    else:\n",
        "        soup=BeautifulSoup(response.text, 'lxml') #response and lxml parser\n",
        "        return soup\n",
        "\n",
        "def get_detail_data(soup):\n",
        "    # title\n",
        "    try:\n",
        "        title=soup.find('h1', id='itemTitle').get_text()[16:]\n",
        "    except:\n",
        "        title=\"\"\n",
        "\n",
        "    # currency & price\n",
        "    try:\n",
        "        try:\n",
        "            p=soup.find('span',id=\"prcIsum\").get_text()\n",
        "        except:\n",
        "            p=soup.find('span',id=\"mm-saleDscPrc\").get_text()\n",
        "        cur_reg=re.compile(\"[a-zA-Z\\s$]+\")\n",
        "        currency=cur_reg.findall(p)[0]\n",
        "        price_reg=re.compile(\"\\d+.\\d+\")\n",
        "        price=price_reg.findall(p)[0]\n",
        "    except:\n",
        "        currency=\"\"\n",
        "        price=\"\"\n",
        "\n",
        "    # item sold\n",
        "    try:\n",
        "        sold=soup.find('span',{'class':'w2b-sgl'}).get_text().split(' ')\n",
        "        if \"sold\" not in sold:\n",
        "            sold=soup.find('span',{'class':'vi-txt-underline'}).get_text().split(' ')[0]\n",
        "        else:\n",
        "            sold=sold[0]\n",
        "    except:\n",
        "        sold=\"\"\n",
        "\n",
        "    # data in dictionary\n",
        "    data={\n",
        "        'title':title,\n",
        "        'price':price,\n",
        "        'currency':currency,\n",
        "        'total sold':sold\n",
        "    }\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_index_data(soup):\n",
        "    try:\n",
        "        links=soup.findAll('a',{\"class\":\"s-item__link\"})\n",
        "    except:\n",
        "        links=''\n",
        "\n",
        "    urls=[item.get('href') for item in links]\n",
        "    return urls\n",
        "\n",
        "\n",
        "def write_csv(data, url):\n",
        "    with open('output.csv','a') as csvfile:\n",
        "        writer=csv.writer(csvfile)\n",
        "        row=[data['title'], data['price'], data['currency'], data['total sold'],\n",
        "             url]\n",
        "        writer.writerow(row)\n",
        "\n",
        "\n",
        "# manage calls from other functions and collect scrapped data\n",
        "def main():\n",
        "    url=\"https://www.ebay.com/sch/i.html?_nkw=book\"\n",
        "    products=get_index_data(get_page(url))\n",
        "    \n",
        "    for link in products:\n",
        "        data = get_detail_data(get_page(link))\n",
        "        write_csv(data,link)\n",
        "\n",
        "\n",
        "\n",
        "# create entry point for scraper\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}